######################################kafka的操作###############################################################################
/usr/soft/kafka_2.11-0.10.0.0/bin/kafka-server-start.sh  -daemon /home/hadoop/kafka_2.11/config/server.properties

kafka-server-start.sh   /home/hadoop/kafka_2.11/config/server.properties

##查看所有topic##
/usr/soft/kafka_2.11-0.10.0.0/bin/kafka-topics.sh --list --zookeeper  66.6.51.248:2181

##删除topic##
/usr/soft/kafka_2.11-0.10.0.0/bin/kafka-topics.sh --delete --zookeeper 66.6.51.248:2181 --topic event_log

##创建topic##
/usr/soft/kafka_2.11-0.10.0.0/bin/kafka-topics.sh --create --zookeeper 192.168.1.182:2181 --replication-factor 1 --partitions 1 --topic gs_bigdata_event_realtime

##客户日志生产者##
/usr/soft/kafka_2.11-0.10.0.0/bin/kafka-console-producer.sh --broker-list 66.6.51.248:9092 --topic accesslog_logstash

##客户日志消费者##
/usr/soft/kafka_2.11-0.10.0.0/bin/kafka-console-consumer.sh --zookeeper 66.6.51.248:2181  --topic test_gs_ty_abnormal 
/usr/soft/kafka_2.11-0.10.0.0/bin/kafka-console-consumer.sh --zookeeper 66.6.51.248:2181 --from-beginning --topic gs_bigdata_event_realtime

太仓 tcI0  32.76.4.7（传感器）
省联社 TKHM  32.3.164.40(传感器)                   32.3.164.42(分析)   
紫金  xO3Y   32.50.9.52（传感器）
昆山 w20E   132.9.42.105（传感器）



kafka-console-producer.sh --broker-list 66.6.51.248:9092 --topic 360_liuliang_all <gs_ty_syslog_inner.log

kafka-console-producer.sh --broker-list 66.6.51.248:9092 --topic gs_rizhiyi_waf<waf-19-10-17-10:40:01.txt

 kafka-topics.sh --alter --zookeeper node2:2181 --topic ty_dns --partitions 3
 
 kafka-topics.sh --create --zookeeper node2:2181 --replication-factor 1 --partitions 1 --topic ff
 

/usr/hdp/3.0.0.0-1634/kafka/bin/kafka-topics.sh --list   --zookeeper dev-3:2181

/usr/hdp/3.0.0.0-1634/kafka/bin/kafka-topics.sh --describe --zookeeper 66.6.51.248:2181 --topic event_log

/usr/hdp/3.0.0.0-1634/kafka/bin/kafka-topics.sh --zookeeper 66.6.51.248:2181 --alter --partitions 4  --replication-factor 2  --topic event_log


/usr/hdp/3.0.0.0-1634/kafka/bin/kafka-topics.sh --create --zookeeper dev-3:2181 --replication-factor 2 --partitions 2 --topic test_log

hbase org.apache.hadoop.hbase.mapreduce.Driver export Dfwybank_VCloude_BusinessJustice 1.txt


flume-ng agent -c . -f kafka_to_hdfs.conf -n a1 -Dflume.root.logger =INFO,console


flume-ng agent -n a1 -f /home/hadoop/shell/l2k.conf -Dflume.root.logger =INFO,console &


flume-ng agent -n a1 -f /home/hadoop/shell/l2k.conf -Dflume.root.logger =INFO,console &
#!/bin/bash
nohup  flume-ng agent -n a1 -c conf -f /home/hadoop/flume/conf/ty/gs_ty_abnormal.conf -Dflume.root.logger=INFO,console &
nohup  flume-ng agent -n a1 -c conf -f /home/hadoop/flume/conf/ty/gs_ty_db.conf -Dflume.root.logger=INFO,console &
nohup  flume-ng agent -n a1 -c conf -f /home/hadoop/flume/conf/ty/gs_ty_dns.conf -Dflume.root.logger=INFO,console &
nohup  flume-ng agent -n a1 -c conf -f /home/hadoop/flume/conf/ty/gs_ty_file.conf -Dflume.root.logger=INFO,console &
nohup  flume-ng agent -n a1 -c conf -f /home/hadoop/flume/conf/ty/gs_ty_ftpop.conf -Dflume.root.logger=INFO,console &
nohup  flume-ng agent -n a1 -c conf -f /home/hadoop/flume/conf/ty/gs_ty_httpflow.conf -Dflume.root.logger=INFO,console &
nohup  flume-ng agent -n a1 -c conf -f /home/hadoop/flume/conf/ty/gs_ty_ids.conf -Dflume.root.logger=INFO,console &
#nohup  flume-ng agent -n a1 -c conf -f /home/hadoop/flume/conf/ty/gs_ty_ioc.conf -Dflume.root.logger=INFO,console &
nohup  flume-ng agent -n a1 -c conf -f /home/hadoop/flume/conf/ty/gs_ty_ldap.conf -Dflume.root.logger=INFO,console &
nohup  flume-ng agent -n a1 -c conf -f /home/hadoop/flume/conf/ty/gs_ty_login.conf -Dflume.root.logger=INFO,console &
nohup  flume-ng agent -n a1 -c conf -f /home/hadoop/flume/conf/ty/gs_ty_mail.conf -Dflume.root.logger=INFO,console &
nohup  flume-ng agent -n a1 -c conf -f /home/hadoop/flume/conf/ty/gs_ty_ssl.conf -Dflume.root.logger=INFO,console &
nohup  flume-ng agent -n a1 -c conf -f /home/hadoop/flume/conf/ty/gs_ty_tcp.conf -Dflume.root.logger=INFO,console &
#nohup  flume-ng agent -n a1 -c conf -f /home/hadoop/flume/conf/ty/gs_ty_telnet.conf -Dflume.root.logger=INFO,console &
nohup  flume-ng agent -n a1 -c conf -f /home/hadoop/flume/conf/ty/gs_ty_udp.conf -Dflume.root.logger=INFO,console &
nohup  flume-ng agent -n a1 -c conf -f /home/hadoop/flume/conf/ty/gs_ty_webattack.conf -Dflume.root.logger=INFO,console &

/home/hadoop/spark-2.4.0-bin-hadoop2.6/bin/spark-submit   --driver-memory 2g  --executor-memory 3g  --executor-cores 2  --conf spark.yarn.am.memory=1024m  --master yarn-client    --conf spark.io.compression.codec=snappy  /home/hadoop/shell/spark/sparkes-1.0-SNAPSHOT.jar 


以下为个人经验总结
       迁移主要步骤详解见分隔符以上内容
       主要有几点补充：
          1、增加节点后，原来的topic partition并不会主动进入新节点上，主要重新分配partition。假如之前有三个节点1、2、3（老），后来扩充4、5、6（新）节点。需要把老的节点上部分topic迁移到新节点上，那么在分配时应这么写：
kafka-reassign-partitions.sh --zookeeper 10.162.1.250:2181,10.162.1.251:2181,10.162.1.252:2181 --topics-to-move-json-file topics-to-move.json --broker-list "4,5,6" --generate

如果是需要在6个节点上重新分配，那么是：

kafka-reassign-partitions.sh --zookeeper 10.162.1.250:2181,10.162.1.251:2181,10.162.1.252:2181 --topics-to-move-json-file topics-to-move.json --broker-list "1,2,3,4,5,6" --generate

          2、如果是在原来节点上扩增磁盘，那个重新分配partition操作和新增节点一样，直接指定broker做generate分配，分区会自动分配在新目录上。
          3、最后一点很重要，如果某一个节点上某个磁盘空间已经很饱和，比如某个磁盘利用率已经到90%，另外其他盘利用率只有百分之几或者是新增的盘还未使用，那么重新分配partition时最好要先把部分topic迁到别的节点上，执行generate时不要写该节点，如果写上该节点，迁移速度很慢，并且可能迁移过程中已经快满的目录很快就100%占用，broker节点相应会挂掉。正常情况下，迁移的逻辑是先把老的partition日志文件目录拷贝在新目录上，然后在删除老的partition日志目录，但是有时候由于节点负载过高，迁移完成后，老partition日志目录并不会自己删除，确定迁移正确完成后，可以人为的删除即可。个人的处理方式是：先把部分topic迁移到其他节点上，磁盘利用率降下来后，在操作其他剩余topic重新分配在全部节点（包括先前磁盘利用率很高的节点）。
 
自动生成分配计划：kafka-reassign-partitions.sh --zookeeper 10.X.X.X:2181,10.X.X.X:2181,10.X.X.X:2181 --topics-to-move-json-file topics-to-move.json --broker-list "1,2,3,4,5,6,7,8,9,10" --generate

执行分配计划：kafka-reassign-partitions.sh --zookeeper 10.X.X.X:2181,10.X.X.X:2181,10.X.X.X:2181 --reassignment-json-file topic-reassignment.json --execute

查看迁移进度：kafka-reassign-partitions.sh --zookeeper 10.X.X.X:2181,10.X.X.X:2181,10.X.X.X:2181 --reassignment-json-file topic-reassignment.json --verify

均衡：kafka-preferred-replica-election.sh --zookeeper 10.X.X.X:2181,10.X.X.X:2181,10.X.X.X:2181

nohup hiveserver2 1>/home/hadoop/hive-1.1.0-cdh5.7.0/logs/hiveserver.log 2>/home/hadoop/hive-1.1.0-cdh5.7.0/logs/hiveserver.err &


beeline -u jdbc:hive2://66.6.51.29:10000 -n root
beeline -u jdbc:hive2://node2:10000 -n root
java -cp tai-1.0-SNAPSHOT.jar com.exec.TaiRun4LocationStats > 20191231.log



kafka-producer-perf-test.sh --topic test --num-records 100 --record-size 1 --throughput 100 --producer-props bootstrap.servers=localhost:9092