package com.dfwy.online.sparkstreamingtask.vcloude.main;


import com.alibaba.fastjson.JSONObject;
import com.dfwy.online.sparkstreamingtask.business.basics.IdeQuotasBusiService;
import com.dfwy.online.sparkstreamingtask.business.basics.IdeQuotasBusiServiceImpl;
import com.dfwy.online.sparkstreamingtask.vcloude.task.*;
import com.dfwy.online.sparkstreamingtask.vcloude.task.etlclean.AmarsoftETLInsertData;
import com.dfwy.online.sparkstreamingtask.vcloude.task.etlclean.CCXETLInsertData;
import com.dfwy.online.sparkstreamingtask.vcloude.task.etlclean.ElementsCreditETLInsertData;
import common.pojo.applicantinformation.ApplicantInformationTO;
import common.pojo.quotas.QuotasDataValueEntity;
import common.utils.exception.ErrorCodeIDE;
import common.utils.exception.ServiceException;
import common.utils.stringutils.StringUtils;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.spark.SparkConf;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka010.ConsumerStrategies;
import org.apache.spark.streaming.kafka010.KafkaUtils;
import org.apache.spark.streaming.kafka010.LocationStrategies;

import java.util.*;

import org.apache.kafka.common.serialization.StringDeserializer;

import java.util.Collection;

import static common.constant.Constants.Dfwybank_VCloude_SparkStreaming_Kafka_Consumer_Topics;
import static common.constant.Constants.Dfwybank_VCloude_Kafka_Bootstrap_Servers;
import static common.constant.Constants.Dfwybank_VCloude_SparkStreaming_BJAppName;

/**
 * 在auto.offset.reset标志位为earliest/或者latest，
 * 上面的消费者都将在enable.auto.commit的基础之上进行继续消费（如果enable.auto.commit=TRUE，
 * 则同一个消费者重启之后不会重复消费之前消费过的消息；
 * enable.auto.commit=FALSE，则消费者重启之后会消费到相同的消息）
 * <p>
 * 原来这是对新的消费者而言（什么是新的消费者？比如说修改了上面消费者的GROUP_ID标识位）
 * 在auto.offset.reset=earliest情况下，新的消费者（消费者二）将会从头开始消费Topic下的消息
 * 在auto.offset.reset=latest情况下，新的消费者将会从其他消费者最后消费的offset处（offset=40开始）开始消费Topic下的消息
 */
public class JavaDirectKafkaAnalyze {

    public static void main(String[] args) throws Exception {
        //SparkConf的配置信息
        //SparkConf conf = new SparkConf().setAppName(Dfwybank_VCloude_SparkStreaming_BJAppName);
        SparkConf conf = new SparkConf().setAppName(Dfwybank_VCloude_SparkStreaming_BJAppName).setMaster("local[6]");
        JavaStreamingContext jssc = new JavaStreamingContext(conf, Durations.seconds(5));
        //Kafka的配置信息
        Map<String, Object> kafkaParams = new HashMap<>();
        kafkaParams.put("bootstrap.servers", Dfwybank_VCloude_Kafka_Bootstrap_Servers);
        kafkaParams.put("key.deserializer", StringDeserializer.class);
        kafkaParams.put("value.deserializer", StringDeserializer.class);
        kafkaParams.put("group.id", "StoreData");
        //auto.offset.reset针对于对新的消费者而言，
        //是从头消费还是从从其他消费者最后消费的offset处开始消费
        kafkaParams.put("auto.offset.reset", "latest");//latest  earliest
        kafkaParams.put("enable.auto.commit", true);
        //上产上线平台使用的
        //Collection<String> topics = Dfwybank_VCloude_SparkStreaming_Kafka_Consumer_Topics;
        //测试平台使用的
        Collection<String> topics = Dfwybank_VCloude_SparkStreaming_Kafka_Consumer_Topics;
        //虚拟机使用的
        //Collection<String> topics = Arrays.asList(Dfwybank_VCloude_SparkStreaming_Kafka_Consumer_Topics);
        JavaInputDStream<ConsumerRecord<String, String>> stream = KafkaUtils.createDirectStream(
                jssc,
                LocationStrategies.PreferConsistent(),
                ConsumerStrategies.<String, String>Subscribe(topics, kafkaParams)
        );
        //获取KV键值对中的V值，因为K值是null是不需要的
        JavaDStream<String> javaDStream = stream.map(record -> record.value());
        //进行数据的处理！
        javaDStream.foreachRDD(rdd -> {
            rdd.foreach(record -> {
                //数据的解析和HBase储存
                if (!(record == null || record.equals(""))) {
                    //System.out.println(record);
                    //1、将传输过来的报文数据封装进申请人实例中，做数据的传输！
                    ApplicantInformationTO applicantInformationTO = JsonAnalysis.json2Object(record);
                    //2、原始报文的数据清洗入库工作
                    if (StringUtils.isNotEmpty(applicantInformationTO.getAmarsoftData())){
                        AmarsoftETLInsertData.etlClean(applicantInformationTO);
                    }else if(StringUtils.isNotEmpty(applicantInformationTO.getElementscreditData())){
                        ElementsCreditETLInsertData.etlClean(applicantInformationTO);
                    }else if(StringUtils.isNotEmpty(applicantInformationTO.getCcxData())){
                        CCXETLInsertData.etlClean(applicantInformationTO);
                    }else {
                        throw new ServiceException(ErrorCodeIDE.VCloudeSpark.Date_Source_From + "：数据来源数据为空！");
                    }
                    //3、指标计算接口
                    IdeQuotasBusiService ideQuotasBusiService = new IdeQuotasBusiServiceImpl();
                        //3.1指标计算值对象集合获得
                    Vector<QuotasDataValueEntity> dataValueList = ideQuotasBusiService.execQuotas(applicantInformationTO, "A", "1");
                        //3.2拼接发送计算结果数据Json
                    String jsonStr = JsonSplice.getJsonStr(dataValueList);
                    //4、报文和计算结果存入HBase数据库 applicantInformationTO  dataValueList
                    HBaseOperater.data2HBase(record, applicantInformationTO, jsonStr);
                    //5、Http请求的发送
                    Integer code = 0;
                    String msg = SendDataToKafka.sendMessage(jsonStr);
                    JSONObject jsonObject = JSONObject.parseObject(msg);
                    code = jsonObject.getInteger("code");
                    if (!(code == 0)) {
                        throw new ServiceException(ErrorCodeIDE.VCloudeSpark.Spark_Kafka_Producer_Send_ERROR + "：Kafka生产者发送失败！");
                    }
                }
            });
        });
        jssc.start();
        jssc.awaitTermination();
    }
}





